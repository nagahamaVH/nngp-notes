---
title: "Nearest Neighbor Gaussian Process"
author: "Victor Hugo Nagahama"
output: pdf_document
bibliography: references.bib
---

## Spatial regression model

Considering a spatial data and $\boldsymbol{s}_i$ denoting the $i^{th}$ observed location then a spatial linear mixed models with response $y(\boldsymbol{s}_i)$ and predictor $\boldsymbol{x}(\boldsymbol{s}_i)$ of dimension $p \times 1$ is defined as:

\begin{equation*}
y(\boldsymbol{s}_i) = \boldsymbol{x}(\boldsymbol{s}_i)^\top \boldsymbol{\beta} + w(\boldsymbol{s}
_i) + \epsilon(\boldsymbol{s}_i),
\end{equation*}

where $\boldsymbol{\beta}$ is the $p \times 1$ vector, $\epsilon(\boldsymbol{s}_i)$ is the random noise assumed as an independent and identically distributed distribution (iid) from a $N(0, \tau^2)$ and $w(\boldsymbol{s}_i)$ is a latent variable that captures the spatial effect. 

Gaussian Process (GP) are typically used to model the spatial surface as $w(\boldsymbol{s}) \sim GP(0, C(\cdot,\cdot \mid \boldsymbol{\theta}))$ where $C(\cdot, \cdot \mid \boldsymbol{\theta})$ is a covariance function with parameters $\boldsymbol{\theta}$. So the random effects $\boldsymbol{w} = (w(\boldsymbol{s}_1), \dots, w(\boldsymbol{s}_n))$ follows a zero-mean multivariate Gaussian distribution with covariance matrix $\boldsymbol{C}(\boldsymbol{\theta}) = (c_{ij})$ where $c_{ij} = C(\boldsymbol{s}_i, \boldsymbol{s}_j \mid \boldsymbol{\theta})$.

Estimating the parameters of the dense covariance matrix can be computational expensive especially when $n$ is large. The basic idea of Nearest Neighbor Gaussian Process (NNGP) is to make the covariance matrix sparse.

## Nearest Neighbor Gaussian Process (NNGP)

The joint distribution of $\boldsymbol{w}$ can be written as the product of conditional densities, that is

\begin{equation}
p(\boldsymbol{w}) = p(w_1) \times p(w_2 \mid w_1) \times p(w_3 \mid w_1, w_2) \times \dots \times p(w_n \mid w_1, w_2, \dots, w_{n-1}).
\label{pdf w}
\end{equation}

If $n$ is large then the conditional densities $p(w_i)$ has superfluous and/or redundant information in a way that can be replaced by at most $m$ observations, where $m \ll n$ [@Vecchia1988]. In fact, finding the best subset of $m$ locations is a non convex optimization problem and the Vecchia's approximation selecting the $m$ nearest points from $s_i$, preserving the locations with highest correlations, have performed extremely well in wide range of simulation studies [@Datta2016].

For each location $i$ we have the neighbor set $N(s_i)$ defined as the $m$ nearest neighbors of $s_i$ then the Equation \ref{pdf w} is simplified to

\begin{equation}
\tilde{p}(\boldsymbol{w}) = \prod_{i=1}^n p\left( w_i \mid w_{N(s_i)} \right),
\label{pdf proxy w}
\end{equation}

which $\tilde{p}(\boldsymbol{w})$ is a joint density of a multivariate Gaussian distribution with a sparse covariance matrix $\tilde{\boldsymbol{C}}(\boldsymbol{\theta})$ almost indistinguishable from $\boldsymbol{C}(\boldsymbol{\theta})$. Furthermore $N(s_i)$ can be defined as follow

\begin{align*}
N(s_1) & = \{\} \; \textrm{(empty set)}, \\
N(s_i) & = \min(m, i - 1) \; \textrm{nearest neighbors of} \; s_i \; \textrm{in} \; s_1, \dots, s_{i - 1} \textrm{for} \; i = 2, \dots, n.
\end{align*}

The space locations are not naturally ordered, so it requires to impose an order, for example by the first coordinate. Regardless the criteria of ordering the approximation of (\ref{pdf proxy w}) to (\ref{pdf w}) has no discernible impact [@Datta2016].


<!-- THEORY -->
<!-- Cholesky decomposition (?) -->
<!-- plot sparse matrix -->
<!-- plots the process of finding nearest neighbor-->
<!--other methods to define subsets-->
<!-- APPLICATION -->
<!-- compare spConjNNGP and stan-->

<!-- - compare full gp and nngp studying difference values of m -->
<!--     - joint likelihood -->
<!--     - consider only spatial effect -->
    
## Full GP and NNGP


## Simulation study

```{r, message=F}
library(dplyr)
library(spNNGP)

rmvn <- function(n, mu = 0, V = matrix(1)){
  p <- length(mu)
  if (any(is.na(match(dim(V), p))))
    stop("Dimension problem!")
  D <- chol(V)
  t(matrix(rnorm(n * p), ncol = p) %*% D + rep(mu, rep(n, p)))
}

set.seed(3582)
n <- 1000
coords <- cbind(runif(n), runif(n))
x <- cbind(1, rnorm(n))

sigma_sq <- 1
tau_sq <- 0.5
phi <- 3 / 0.5

D <- as.matrix(dist(coords))
R <- exp(-phi * D)
C <- sigma_sq * R
w <- rmvn(1, rep(0, n), C)
B <- as.matrix(c(1, 5))
y <- rnorm(n, x %*% B + w, sqrt(tau_sq))

cov_model <- "exponential"
```

```{r}
M <- 5
n_samples <- 200
sigma_sq_IG <- c(2, sigma_sq)

starting <- list("phi"=phi, "sigma.sq"=5, "tau.sq"=1)

tuning <- list("phi"=0.5, "sigma.sq"=0.5, "tau.sq"=0.5)

priors <- list("phi.Unif"=c(3/1, 3/0.01), "sigma.sq.IG"=c(2, 5), "tau.sq.IG"=c(2, 1))

cov.model <- "exponential"

model <- spNNGP(y ~ x-1, coords=coords, starting=starting, method="latent", n.neighbors=M,
              tuning=tuning, priors=priors, cov.model=cov.model,
              n.samples=n_samples,return.neighbor.info = T,  n.omp.threads=2)

round(summary(model$p.beta.samples)$quantiles[,c(3,1,5)],2)
round(summary(model$p.theta.samples)$quantiles[,c(3,1,5)],2)
plot(w, apply(model$p.w.samples, 1, median),  xlab="True w", ylab="Posterior median w")
```



## References
