---
title: "Generalized Gaussian Process Using NNGP"
author: "Victor Hugo Nagahama"
date: "2022-11-22"
output:
  pdf_document:
    toc: true
    number_sections: true
  html_document:
    toc: true
    number_sections: true
editor_options:
  chunk_output_type: console
bibliography: references.bib
---

***

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  attr.source = ".numberLines",
  fig.height = 3,
  fig.width = 7
)
```

Generalized Gaussian Process (GGP) are latent Gaussian Process (GP) which the response variable belong to a distribution of exponential family. Nearest Neighbour Gaussian Process (NNGP) is an alternative for GPs that becomes infeasible for large number of locations.

In this report we review GGP regression and NNGP; and illustrate the estimation procedure of the combined methods in the context of spatial data through a simulation study covering each step in details and then the results of estimation for a more realistic dataset.

# Inference of latent Gaussian Process model

There are two interest quantities that we want to estimate in a latent Gaussian Process model, the latent field $\pi(\boldsymbol{x} \mid \boldsymbol{y})$ and the posterior distribution of parameters $\pi(\boldsymbol{\theta} \mid \boldsymbol{y})$.

Let the vector of parameters $\boldsymbol{\theta}$, then the expression of its posterior distribution can be obtained using conditional probability properties

\begin{equation*}
    \pi(\boldsymbol{\theta} \mid \boldsymbol{y}) \propto \frac{\pi(\boldsymbol{y} \mid \boldsymbol{x}, \boldsymbol{\theta}) \pi(\boldsymbol{x} \mid \boldsymbol{\theta})}{\pi(\boldsymbol{x} \mid \boldsymbol{y}, \boldsymbol{\theta})}.
\end{equation*}

The full conditional density in the denominator has no analytical form. Assuming a known $\boldsymbol{\theta}$ it is possible to rewrite the expression as $\pi(\boldsymbol{x} \mid \boldsymbol{y}) = \pi(\boldsymbol{x}, \boldsymbol{y}) / \pi(\boldsymbol{y})$ and obtain an approximation applying a second-order Taylor expansion (Laplace approximation) on the log joint density at the mode of posterior $\pi(\boldsymbol{x} \mid \boldsymbol{y})$

\begin{align}
    \pi(\boldsymbol{x} \mid \boldsymbol{y}) & = \frac{ \pi(\boldsymbol{x}) \pi(\boldsymbol{y} \mid \boldsymbol{x})}{\pi(\boldsymbol{y})} \nonumber \\
    & \propto \pi(\boldsymbol{x}) \pi(\boldsymbol{y} \mid \boldsymbol{x}) \label{posterior of latent field}.
\end{align}

Iterative applications of the Taylor expansion under regular conditions converges to the desired multivariate normal distribution

\begin{equation*}
  \tilde{\pi}(\boldsymbol{\theta} \mid \boldsymbol{y}) \propto \frac{\pi(\boldsymbol{y} \mid \boldsymbol{x}, \boldsymbol{\theta}) \pi (\boldsymbol{x} \mid \boldsymbol{\theta})}{|\boldsymbol{Q}_{\boldsymbol{x} \mid \boldsymbol{y}}|^{1/2}},
\end{equation*}

where $\boldsymbol{Q}_{\boldsymbol{x} \mid \boldsymbol{y}}$ is the precision matrix of the posterior distribution of latent field.

The parameter vector and the latent field can be estimated jointly using the mentioned expression. In order to do that we need to define the posterior distribution of the parameter, its gradient and hessian function.

# Nearest Neighbour Gaussian Process (NNGP)

We can formalise the definition on NNGP as following. Consider a set of locations $\boldsymbol{S} = \{ \boldsymbol{s}_1, \boldsymbol{s}_2, \dots, \boldsymbol{s}_n \}$ where the data was sampled, for each location $\boldsymbol{s}_i$ we have a subset $\boldsymbol{c}(i)$ defined as the $m$ nearest neighbors of $\boldsymbol{s}_i$ then the approximation of $\pi(\boldsymbol{x})$ is given by

\begin{equation}
    \tilde{\pi}(\boldsymbol{x}) = \prod_{i=1}^n \pi \left( x_i \mid \boldsymbol{x}_{\boldsymbol{c}(i)} \right),
    \label{joint density of nngp}
\end{equation}

where each conditional density follows a normal distribution with parameters

\begin{equation}
    x_i \mid \boldsymbol{x}_{\boldsymbol{c}(i)} \sim \mathcal{N} \left( \boldsymbol{a}_i \boldsymbol{x}_{\boldsymbol{c}(i)}, d_i \right) \quad
    \begin{cases}
        \boldsymbol{a}_i = C(\boldsymbol{s}_i, \boldsymbol{c}(i)) \left(C(\boldsymbol{c}(i), \boldsymbol{c}(i) \right)^{-1} \\
        d_i = C(\boldsymbol{s}_i, \boldsymbol{s}_i) - \boldsymbol{a}_i^T C(\boldsymbol{c}(i), \boldsymbol{s}_i)
    \end{cases}.
    \label{conditional densities of nngp}
\end{equation}

The goal here is to find the parameters of the multivariate normal distribution in \eqref{joint density of nngp}. In order to obtain that, we write the conditional densities as a linear combination of normal densities $\gamma_i \sim \mathcal{N}(0, d_i)$

\begin{align*}
    x_1 & = \gamma_1 \\
    x_2 & = a_{21} x_1 + \gamma_2 \\
    x_3 & = a_{31} x_1 + a_{32} x_2 + \gamma_3 \\
    \vdots & \qquad \vdots \\
    x_n & = a_{n1} x_1 + a_{n2} x_2 + \dots + \gamma_n.
\end{align*}

Stacking the vectors we have

\begin{equation*}
    \boldsymbol{x} = \boldsymbol{A} \boldsymbol{x} + \boldsymbol{\gamma},
\end{equation*}

where $\boldsymbol{A} = (a_{ij})$ is a lower triangular matrix and $\boldsymbol{\gamma} \sim \mathcal{N}(\boldsymbol{0}, \boldsymbol{D})$ with $\boldsymbol{D} = diag(d_1, \dots, d_n)$. Finally we have the specification of \eqref{joint density of nngp} which has a sparse precision matrix and Cholesky decomposition $\boldsymbol{L}^T \boldsymbol{L}$

\begin{align*}
  & \boldsymbol{x} = \boldsymbol{A} \boldsymbol{x} + \boldsymbol{\gamma} \\
  & \Leftrightarrow \boldsymbol{x} - \boldsymbol{A} \boldsymbol{x} = \boldsymbol{\gamma} \\
  & \Leftrightarrow (\boldsymbol{I} - \boldsymbol{A}) \boldsymbol{x} = \boldsymbol{\gamma} \\
  & \Leftrightarrow \boldsymbol{x} = (\boldsymbol{I} - \boldsymbol{A})^{-1} \boldsymbol{\gamma} \\
  & \therefore \boldsymbol{x} \sim \mathcal{N} \left( \boldsymbol{0}, (\boldsymbol{I} - \boldsymbol{A})^{-1} \boldsymbol{D} (\boldsymbol{I} - \boldsymbol{A})^{-T} \right)
\end{align*}

\begin{align}
  \tilde{\boldsymbol{C}}^{-1} & = (\boldsymbol{I} - \boldsymbol{A})^T \boldsymbol{D} (\boldsymbol{I} - \boldsymbol{A}) \nonumber \\
  \boldsymbol{L} & = \boldsymbol{D}^{-1/2} (\boldsymbol{I} - \boldsymbol{A}). \label{cholesky}
\end{align}

# Laplace Approximation

Let a vector of non-normal response $\boldsymbol{y}$ belonging to exponential family and a latent field $\boldsymbol{x} \sim GP(\boldsymbol{\mu}, \boldsymbol{Q})$. Consider $g(\boldsymbol{x}) = \log \pi (\boldsymbol{y} \mid \boldsymbol{x}, \boldsymbol{\theta})$ for a given $\boldsymbol{\theta}$ and for simplicity of notation $\nabla g^{(0)} = \left [ g'(\boldsymbol{x}^{(0)}) \right ]$ and $\boldsymbol{H}^{(0)} = \textrm{diag} \left( g''(\boldsymbol{x}^{(0)}) \right)$

\begin{align*}
    \pi(\boldsymbol{y} \mid \boldsymbol{x}, \boldsymbol{\theta}) & \approx \exp \left\{ g(\boldsymbol{x}^{(0)}) + \left(\boldsymbol{x} - \boldsymbol{x}^{(0)}\right)^T \nabla g^{(0)} + \frac{1}{2} \left(\boldsymbol{x} - \boldsymbol{x}^{(0)}\right)^T \boldsymbol{H}^{(0)} \left(\boldsymbol{x} - \boldsymbol{x}^{(0)}\right) \right\} \\
    & = \exp \left\{ \boldsymbol{x}^T \left( \nabla g^{(0)} - \boldsymbol{H}^{(0)} \boldsymbol{x}^{(0)} \right) - \frac{1}{2} \boldsymbol{x}^T \boldsymbol{H}^{(0)} \boldsymbol{x} \right\} + \textrm{const} \\
    & \approx \exp \left\{ \boldsymbol{x}^T \left( \nabla g^{(0)} - \boldsymbol{H}^{(0)} \boldsymbol{x}^{(0)} \right) - \frac{1}{2} \boldsymbol{x}^T \boldsymbol{H}^{(0)} \boldsymbol{x} \right\}.
\end{align*}

Replacing the above equation in \eqref{posterior of latent field} and rearranging the terms we obtain the Gaussian approximation of the posterior distribution of latent field named $\tilde{\pi}(\boldsymbol{x} \mid \boldsymbol{y}, \boldsymbol{\theta})$.

\begin{align*}
    \tilde{\pi}(\boldsymbol{x} \mid \boldsymbol{y}, \boldsymbol{\theta}) & \propto \pi(\boldsymbol{x} \mid \boldsymbol{\theta}) \pi(\boldsymbol{y} \mid \boldsymbol{x}, \boldsymbol{\theta}) \\
    & \approx  \exp \left\{ \boldsymbol{x}^T \boldsymbol{Q} \boldsymbol{\mu} - \frac{1}{2} \boldsymbol{x}^T \boldsymbol{Q} \boldsymbol{x} \right\} \exp \left\{ \boldsymbol{x}^T \left( \nabla g^{(0)} - \boldsymbol{H}^{(0)} \boldsymbol{x}^{(0)} \right) - \frac{1}{2} \boldsymbol{x}^T \boldsymbol{H}^{(0)} \boldsymbol{x} \right\} \\
    & = \exp \left\{ \boldsymbol{x}^T \left( \nabla g^{(0)} - \boldsymbol{H}^{(0)} \boldsymbol{x}^{(0)} + \boldsymbol{Q} \boldsymbol{\mu} \right) - \frac{1}{2} \boldsymbol{x}^T \left( \boldsymbol{Q} + \boldsymbol{H}^{(0)} \right) \boldsymbol{x} \right\},
\end{align*}

\noindent
which is a multivariate normal distribution with parameters

\begin{align}
    \boldsymbol{\mu}^{(0)}_{\boldsymbol{x} \mid \boldsymbol{y}} & = \boldsymbol{\mu} + \boldsymbol{Q}^{-1}_{\boldsymbol{x} \mid \boldsymbol{y}} \left( \nabla g^{(0)} - \boldsymbol{H}^{(0)} \boldsymbol{x}^{(0)} \right) \label{posterior mean of x} \\
    \boldsymbol{Q}^{(0)}_{\boldsymbol{x} \mid \boldsymbol{y}} & = \boldsymbol{Q} - \boldsymbol{H}^{(0)}. \label{posterior precision matrix of x}
\end{align}

Note that the approximation depends on $\boldsymbol{x}^{(0)}$ and vector of parameters $\boldsymbol{\theta}$, iterated applications of the quadratic approximation under regularity conditions converges to the mode of $\pi (\boldsymbol{x} \mid \boldsymbol{y}, \boldsymbol{\theta})$ and consequently produces a good approximation of its distribution. The method mentioned is actually the application of a Newton-Raphson algorithm on the log posterior of latent field as showed by @zilber2021vecchia.

# Implementation

Consider a data

\begin{align*}
  \boldsymbol{x} & \sim \mathcal{N} \left( \boldsymbol{0}, \boldsymbol{C}(\boldsymbol{\psi}) \right) \\
  y_i \mid \boldsymbol{x} & \overset{iid}\sim Poisson(\eta_i), \quad \eta_i = \exp x_i,
\end{align*}

\noindent
where the covariance matrix $C(\boldsymbol{\psi})$ is the Squared Exponential (SE) kernel in terms of signal variance and length-scale parameterization

\begin{equation*}
    \mathrm{C}(s_i, s_j) = \sigma^2 \exp \left\{ -\frac{(s_i - s_j)^2}{2 l^2} \right\}.
\end{equation*}

```{r}
library(ggplot2)
library(mvtnorm)
```

```{r}
n <- 15
set.seed(163)
coords <- cbind(runif(n), runif(n))

ord <- order(coords[,1])
coords <- coords[ord,]

sigma <- 1.6
l <- .4

d <- dist(coords) |>
  as.matrix()
C <- sigma^2 * exp(-d / (2 * l^2))
x <- rmvnorm(1, sigma = C, method = "chol", checkSymmetry = F) |>
  c()
eta <- exp(x)
y <- rpois(n, eta)
```

The code above simulate `r n` locations in the spatial domain $\mathcal{D} = [0, 1]^2$ following the mentioned data with covariance parameters $\sigma =$ `r round(sigma, 4)` and $l =$ `r round(l, 4)`.

## NNGP: Find the neighbours of each location

For simplicity we are using the function `spConjNNGP()` from the package `spNNGP` to get the $m$ nearest neighbours of each location.

```{r}
get_NN_ind <- function(ind, ind_distM_i, M){
  l <- ifelse(ind < M, ind, M)
  D_i <- rep(0, M);
  D_i[1:l] <- c(ind_distM_i[[ind]])[1:l]
  return(D_i)
}

get_nn <- function(coords, m){
  n <- dim(coords)[1]
  nn_data <- spNNGP::spConjNNGP(
    rep(0, n) ~ 1, coords = coords,
    n.neighbors = m,
    theta.alpha = c("phi" = 5, "alpha" = 0.5),
    sigma.sq.IG = c(2, 1),
    cov.model = "exponential",
    return.neighbor.info = T, fit.rep = F, 
    verbose = F)
  ord <- nn_data$neighbor.info$ord
  nn_idx <- sapply(1:(n - 1), get_NN_ind, nn_data$neighbor.info$n.indx[-1], m) |>
    t()
  
  return(list(ord = ord, nn_idx = nn_idx))
}
```

The code bellow shows the neighbours of the locations 2 to 6, considering a neighbour size equals to 3.

```{r}
nn <- get_nn(coords, 3)
print(nn$nn_idx[1:5,])
```

## NNGP: Compute precision matrix
The function `nngp_chol()` compute the $\boldsymbol{A}$ and $\boldsymbol{D}$ matrices and the Cholesky decomposition of the precision matrix using \eqref{conditional densities of nngp} and \eqref{cholesky}. The main idea of the algorithm is to initialise $\boldsymbol{A} = (\boldsymbol{a}_1, \dots, \boldsymbol{a}_n)$ as matrix of zeros and the vector $\boldsymbol{d} = diag(\boldsymbol{D})$ with $d_1 = C(\boldsymbol{S}_1, \boldsymbol{S}_1)$ then for each location $i = 2, \dots, n$ we update the matrix and vector according to its neighbours.

```{r}
# Compute Cholesky decomposition of the precision matrix returning A, D^{-1} and 
# L matrices. (Q = (I - A)^T D^{-1} (I - A), where L = D^{-1/2} (I - A))
nngp_chol <- function(coords, m, p) {
  n <- dim(coords)[1]
  sigma <- p[1]
  l <- p[2]
  
  # Initialize A and dd = diag(D)
  A <- matrix(rep(0, n^2), n)
  dd <- c(sigma^2, rep(0, n - 1))
  
  # Get the neighbour list
  nn_list <- get_nn(coords, m)
  
  # Parallelize
  for (i in 1:(n - 1)) {
    nn <- nn_list$nn_idx[i, ]
    nn <- nn[nn != 0]
    d <- dist(coords[c(i + 1, nn),]) |>
      as.matrix()
    C <- sigma^2 * exp(-d / (2 * l^2))
    A[i + 1, nn] <- solve(C[-1, -1], C[-1, 1])
    dd[i + 1] <- C[1, 1] - C[1, -1] %*% A[i + 1, nn]
  }
  
  Dinv <- diag(1 / dd, nrow = n)
  L <- tcrossprod(diag(sqrt(1 / dd), nrow = n), t(diag(nrow = n) - A))
  
  l <- list(A = A, Dinv = Dinv, L = L)
  return(l)
}
```

For example, let's calculate for location of index 8. Assuming known parameters and $m = 3$, first we initialise $\boldsymbol{A}$ and $\boldsymbol{d}$ (named `dd` in the code bellow); and obtain the neighbours for each point.

```{r}
# Initialize A and dd = diag(D)
A <- matrix(rep(0, n^2), n)
dd <- c(sigma^2, rep(0, n - 1))

# Get the neighbour list
nn_list <- get_nn(coords, 3)

i <- 8

nn <- nn_list$nn_idx[i, ]
nn <- nn[nn != 0]
nn
```

Then we compute the covariance matrix between the location 9 and its neighbours.

```{r}
d <- dist(coords[c(i + 1, nn),]) |>
  as.matrix()
Cm <- sigma^2 * exp(-d / (2 * l^2))

Cm
```

Considering the neighbours of 9 use \eqref{conditional densities of nngp} to compute $\boldsymbol{a}_9$ and $d_9$ and update $\boldsymbol{A}$ and $\boldsymbol{d}$.

```{r}
A[i + 1, nn] <- solve(Cm[-1, -1], Cm[-1, 1])
dd[i + 1] <- Cm[1, 1] - Cm[1, -1] %*% A[i + 1, nn]
```

Computed $\boldsymbol{a}_8$ and $d_8$:

```{r}
A[i + 1,]
dd[i + 1]
```

After applying the procedure for all $i = 2, \dots, n$ we obtain the computed matrix $\boldsymbol{A}$ and $\boldsymbol{D}$. In images bellow black represents non zero values.

```{r, echo = F}
precision <- data.frame(
  x = 1:n,
  y = 1:n
) |>
  expand.grid()

precision$gp <- solve(C) |>
  as.vector() |>
  magrittr::is_greater_than(0) |>
  as.numeric()

nngp_decomp <- nngp_chol(coords, 3, c(sigma, l))

precision$IA <- as.vector(diag(nrow = n) - nngp_decomp$A) |>
  magrittr::is_greater_than(0) |>
  as.numeric()

precision$Dinv <- as.vector(nngp_decomp$Dinv) |>
  magrittr::is_greater_than(0) |>
  as.numeric()

precision$nngp <- crossprod(nngp_decomp$L) |>
  as.vector() |>
  magrittr::is_greater_than(0) |>
  as.numeric()

precision$gp_cov <- C |>
  as.vector() |>
  as.numeric()

precision$nngp_cov <- crossprod(nngp_decomp$L) |>
  solve() |>
  as.vector() |>
  as.numeric()

matrix_plot <- list(
  geom_tile(),
  scale_fill_gradient(low = "white", high = "black"),
  theme_bw(),
  scale_x_reverse(expand = c(0,0)),
  scale_y_continuous(expand = c(0,0)),
  theme(
    aspect.ratio = 1,
    panel.grid = element_blank(),
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    legend.position = "none",
    plot.title = element_text(hjust = 0.5)
  )
)

p1 <- ggplot(precision, aes(x, y, fill = IA)) +
  matrix_plot +
  labs(x = "", y = "", title = "(I - A)")

p2 <- ggplot(precision, aes(x, y, fill = Dinv)) +
  matrix_plot +
  labs(x = "", y = "", title = expression(D^{-1}))

p3 <- ggplot(precision, aes(x, y, fill = nngp)) +
  matrix_plot +
  labs(x = "", y = "", title = expression(tilde(C)^{-1}))

cowplot::plot_grid(p1, p2, p3, nrow = 1)
```

Also we can compare the full GP (original) precision matrix and the NNGP version.

```{r, echo = F}
p1 <- ggplot(precision, aes(x, y, fill = gp)) +
  matrix_plot +
  labs(x = "", y = "", title = "Full GP")

p2 <- ggplot(precision, aes(x, y, fill = nngp)) +
  matrix_plot +
  labs(x = "", y = "", title = "NNGP")

cowplot::plot_grid(p1, p2, nrow = 1)
```

As expected, the approximation $\tilde{\boldsymbol{C}}$ is almost indistinguishable from its original covariance matrix.

```{r, echo = F}
p1 <- ggplot(precision, aes(x, y, fill = gp_cov)) +
  matrix_plot +
  labs(x = "", y = "", title = expression(C))

p2 <- ggplot(precision, aes(x, y, fill = nngp_cov)) +
  matrix_plot +
  labs(x = "", y = "", title = expression(tilde(C)))

cowplot::plot_grid(p1, p2, nrow = 1)
```

## Laplace Approximation: Compute full posterior of latent field

The function `la()` compute the Laplace Approximation of the posterior of latent field.

```{r}
# Laplace Approximation of pi(y | x)
la <- function(x0, y, Cinv, grad, hess, control = list(it = 100, tol = 1e-4)) {
  for (i in 1:control$it) {
    # Rearanging means
    H <- diag(hess(x0), length(x0))
    G <- grad(x0, y)
    Cx <- chol2inv(chol(Cinv - H)) # improve?
    mu <- as.vector(Cx %*% (G - H %*% x0))
    
    # N-R algorithm
    # U <- grad(x0, y)
    # D <- diag(-1 / hess(x0), length(x0))
    # Dinv <- diag(-hess(x0), length(x0))
    # Winv <- chol2inv(chol(Cinv + Dinv))
    # t <- x0 + D %*% U
    # mu <- Winv %*% (Dinv %*% t)
    
    if (max(abs(x0 - mu)) < control$tol) break
    if (i == control$it) stop("Max iteration number reached")
    x0 <- mu
  }

  parms <- list(mu = mu, sigma = Cx)
  return(parms)
}
```

For simplicity let's show the approximation for a univariate setting $y \mid x \sim Poisson(\eta)$ and $x \sim \mathcal{N}(0, \sigma^2)$ where $\eta = \exp(x)$ and the parameter is known. The plot bellow shows the exact density obtained from the posterior (continuous) and its Laplace Approximation (dashed), the dot represents the value which the Taylor expansion is centered, that is $\hat{x}$.

```{r, echo = F, fig.height = 4}
C <- diag(2.5, ncol = 1)

set.seed(163)
x <- rmvnorm(1, sigma = C, method = "chol", checkSymmetry = F) |>
  c()
eta <- exp(x)
y <- rpois(1, eta)

posterior <- function(y, x, sigma_x, log = T) {
  eta <- exp(x)
  ll <- sum(dpois(y, eta, log = T)) + dmvnorm(x, sigma = sigma_x, log = T, checkSymmetry = F)
  if (!log) {
    ll <- exp(ll)
  }
  return(ll)
}

grad <- function(x, y) {
  y - exp(x)
}

hess <- function(x) {
  -exp(x)
}

la_example <- function(x0, y, Cinv, grad, hess, it) {
  for (i in 1:it) {
    H <- diag(hess(x0), length(x0))
    G <- grad(x0, y)
    Cx <- chol2inv(chol(Cinv - H)) # improve?
    mu <- as.vector(Cx %*% (G - H %*% x0))
    x0 <- mu
  }
  parms <- list(mu = mu, sigma = Cx)
  return(parms)
}

Cinv <- solve(C)
proxy <- la(log(y + .5), y, Cinv, grad, hess)

h <- 0.05
x_grid <- seq(
  proxy$mu - 3 * c(proxy$sigma), proxy$mu + 3 * c(proxy$sigma), by = h)

exact <- sapply(x_grid, posterior, y = y, sigma_x = C, log = F)

df <- data.frame(
  x = x_grid,
  exact = exact / sum(exact * h),
  la = dnorm(x_grid, mean = proxy$mu, sd = sqrt(proxy$sigma)) 
)

max_it <- 3
proxy_grid <- sapply(
  1:max_it, la_example, x0 = .9, y = y, Cinv = Cinv, grad = grad, hess = hess)

p_list <- list()
for (i in 1:max_it) {
  mu <- proxy_grid[,i]$mu
  sd <- sqrt(proxy_grid[,i]$sigma[1, 1])
  df <- df |>
    dplyr::mutate(
      it = dnorm(x_grid, mu, sd)
    )
  
  p <- ggplot(df, aes(x = x, y = exact)) +
    geom_line() +
    geom_line(aes(y = it), linetype = "dashed") +
    geom_point(aes(x = mu, y = 0)) +
    labs(y = "Density", title = paste0("Iteration ", i)) +
    theme(plot.title = element_text(hjust = 0.5))
  
  p_list[[i]] <- p
}

p_list[[i + 1]] <- ggplot(df, aes(x = x, y = exact)) +
  geom_line() +
  geom_line(aes(y = la), linetype = "dashed") +
  geom_point(aes(x = proxy$mu, y = 0)) +
  labs(y = "Density", title = "Last iteration") +
  theme(plot.title = element_text(hjust = 0.5))

cowplot::plot_grid(plotlist = p_list, ncol = 2)
```

## Wrapping everything in a single function

The function `la_nngp()` compute the precision matrix of NNGP and the Laplace Approximation returning the log likelihood of the Poisson-NNGP model given a vector of parameters.

```{r}
# Estimating parameters
la_nngp <- function(y, coords, p0, x0, grad, hess, m) {
  print(exp(p0))

  # Vecchia Approx
  L <- nngp_chol(coords, m, exp(p0))

  Cinv <- crossprod(L)
  C <- chol2inv(chol(Cinv))

  # Laplace Approximation
  gauss <- la(x0, y, Cinv, grad, hess)
  x_hat <- gauss$mu
  Cx <- gauss$sigma
  
  # Log-likelihood
  denom <- -(length(y) / 2) * log(2 * pi) - 0.5 * determinant(Cx)$modulus[1]
  ll <- posterior(y, x_hat, C) - denom
  return(ll)
}
```

# Simulation study

```{r}
n <- 500
m <- 10
set.seed(163)
coords <- cbind(runif(n), runif(n))

ord <- order(coords[,1])
coords <- coords[ord,]

sigma <- 2.6
l <- .4

d <- dist(coords) |>
  as.matrix()
C <- sigma^2 * exp(-d / (2 * l^2))
x <- rmvnorm(1, sigma = C, method = "chol", checkSymmetry = F) |>
  c()
eta <- exp(x)
y <- rpois(n, eta)
```

We have to define the log posterior of the parameters, the gradient and hessian function of the data likelihood.

```{r}
# pi(theta | x, y) \propto pi(y | x, theta) * pi(x | theta)
posterior <- function(y, x, sigma_x, log = T) {
  eta <- exp(x)
  ll <- sum(dpois(y, eta, log = T)) + dmvnorm(x, sigma = sigma_x, log = T, checkSymmetry = F)
  if (!log) {
    ll <- exp(ll)
  }
  return(ll)
}

grad <- function(x, y) {
  y - exp(x)
}

hess <- function(x) {
  -exp(x)
}
```

```{r, eval = F}
p0 <- log(c(5, 5))
x0 <- log(y + .5)

fitted_parms <- optim(
  p0, la_nngp, y = y, coords = coords, x0 = x0, grad = grad, hess = hess, m = m,
  method = "Nelder-Mead", hessian = T, control = list(fnscale = -1))
```

```{r echo=FALSE, out.width = "80%"}
knitr::include_graphics("../images/par_poisson_nngp.png")
```

<!-- 
plot latent field
plot y_hat
-->

# References

<!-------------------------- SANITY CHECKS ------------------------------->
```{r check_la_nngp_optim, eval = F, echo = F}
p0 <- log(c(5, 5))
x0 <- log(y + .5)

fitted_parms <- optim(
  p0, la_nngp, y = y, coords = coords, x0 = x0, grad = grad, hess = hess, m = m,
  method = "Nelder-Mead", hessian = T, control = list(fnscale = -1))

sigma_hat <- msm::deltamethod(
  list(~exp(x1), ~exp(x2)), fitted_parms$par, -solve(fitted_parms$hessian), 
  ses = F)

confint2 <- function(mu, sigma, alpha = 0.05) {
  q <- qnorm(alpha / 2) |> 
    abs()
  ub <- mu + q * sqrt(diag(sigma))
  lb <- mu - q * sqrt(diag(sigma))
  ci <- data.frame(ub, lb, mu)
  return(ci)
}

ci_parms <- confint2(exp(fitted_parms$par), sigma_hat) |>
  dplyr::mutate(
    par = c("sigma", "l"),
    true = c(sigma, l)
  )

ggplot(data = ci_parms, aes(x = "")) +
  facet_wrap(~par, scales = "free") +
  geom_hline(aes(yintercept = true), lty = 2) +
  geom_linerange(aes(ymin = lb, ymax = ub)) +
  geom_point(aes(y = mu)) +
  labs(y = "", x = "", title = "Estimated parameter")
# cowplot::save_plot(filename = "./images/par_poisson_nngp.png", plot = last_plot())
```

```{r check_chol, eval = F, echo = F}
n <- 50
m <- n - 1
set.seed(126)
coords <- cbind(runif(n), runif(n))

ord <- order(coords[,1])
coords <- coords[ord,]

sigma <- 2
l <- 1 / 2

d <- dist(coords) |>
  as.matrix()
# d <- dist(coords, diag = T, upper = T)
C <- sigma^2 * exp(-d / (2 * l^2))
x <- rmvnorm(1, sigma = C, method = "chol", checkSymmetry = F) |>
  c()
eta <- exp(x)
y <- rpois(n, eta)

Cinv <- chol2inv(chol(C))

L <- nngp_chol(coords, m, c(sigma, l))
Q_nngp <- crossprod(L)

lm(Q_nngp[lower.tri(Q_nngp)] ~ Cinv[lower.tri(Q_nngp)])
```

```{r check_la_marginal, eval = F, echo = F}
n <- 100
m <- 3
set.seed(126)
coords <- cbind(runif(n), runif(n))

ord <- order(coords[,1])
coords <- coords[ord,]

sigma <- 2
l <- 1 / 2

d <- dist(coords) |>
  as.matrix()
# d <- dist(coords, diag = T, upper = T)
C <- sigma^2 * exp(-d / (2 * l^2))
x <- rmvnorm(1, sigma = C, method = "chol", checkSymmetry = F) |>
  c()
eta <- exp(x)
y <- rpois(n, eta)

x0 <- log(y + .5)
Cinv <- chol2inv(chol(C))
fit_proxy <- la(x0, y, Cinv, grad, hess)

h <- 0.05
n_sample <- 6
set.seed(523)
sampled <- sample(1:n, n_sample, replace = F)
plots <- list()

posterior_marginal <- function(y, mu, x, C) {
  eta <- exp(mu)
  dpois(y, eta) * dmvnorm(x, sigma = C, checkSymmetry = F) 
}

for (i in 1:n_sample) {
  id <- sampled[i]
  se <- fit_proxy$sigma[id, id] |>
    sqrt()
  x_grid <- seq(fit_proxy$mu[id] - 3 * se, fit_proxy$mu[id] + 3 * se, by = h)
  
  df_plot <- rep(x, each = length(x_grid)) |>
    matrix(ncol = n) |>
    dplyr::as_tibble(.name_repair = make.names)
  df_plot[,id] <- x_grid
  names(df_plot) <- paste0("x", 1:n)
  
  unnorm_exact <- apply(df_plot, 1, posterior_marginal, y = y[id], mu = x[id], C = C)
  unnorm_proxy <- apply(df_plot, 1, dmvnorm, mean = fit_proxy$mu, sigma = fit_proxy$sigma, checkSymmetry = F) 
  
  df_plot <- df_plot |>
    dplyr::mutate(
      exact = unnorm_exact / sum(unnorm_exact),
      proxy = unnorm_proxy / sum(unnorm_proxy)
    )

  p <- ggplot(df_plot, aes_string(x = paste0("x", id))) +
    geom_line(aes(y = exact)) +
    geom_line(aes(y = proxy), linetype = 2, col = "red") +
    geom_point(data = data.frame(x = fit_proxy$mu[id]), aes(x = x, y = 0))
  plots[[i]] <- p
}

cowplot::plot_grid(plotlist = plots, nrow = 2)
```

```{r check_la_bivar, eval = F, echo = F}
n <- 50
m <- n - 1
set.seed(126)
coords <- cbind(runif(n), runif(n))

ord <- order(coords[,1])
coords <- coords[ord,]

sigma <- 2
l <- 1 / 2

d <- dist(coords) |>
  as.matrix()
# d <- dist(coords, diag = T, upper = T)
C <- sigma^2 * exp(-d / (2 * l^2))
x <- rmvnorm(10000, sigma = C, method = "chol", checkSymmetry = F) |>
  c()
x <- rmvnorm(10000, sigma = C, method = "chol", checkSymmetry = F) |>
  colMeans()
eta <- exp(x)
y <- rpois(n, eta)

x0 <- log(y + .5)
Cinv <- chol2inv(chol(C))
fit_proxy <- la(x0, y, Cinv, grad, hess)

df_plot <- NULL
w_grid <- list()
h <- 0.05
for (i in 1:n) {
  se <- fit_proxy$sigma[i, i] |>
    sqrt()
  w_grid[[i]] <- seq(fit_proxy$mu[i] - 3 * se, fit_proxy$mu[i] + 3 * se, by = h)
}

df_plot <- expand.grid(w_grid) |>
  as.data.frame()
names(df_plot) <- paste0("x", 1:n)

unnorm_exact <- apply(df_plot, 1, posterior, y = y, sigma_x = C, log = F)
unnorm_proxy <- apply(df_plot, 1, dmvnorm, fit_proxy$mu, fit_proxy$sigma) 

df_plot <- df_plot |>
  dplyr::mutate(
    exact = unnorm_exact / sum(unnorm_exact),
    proxy = unnorm_proxy / sum(unnorm_proxy)
  )

df_longer <- df_plot |>
  tidyr::pivot_longer(cols = c(exact, proxy), names_to = "type", values_to = "density")

ggplot(df_longer, aes(x = x1, y = x2)) +
  geom_contour(aes(z = density)) +
  facet_wrap(vars(type)) +
  geom_point(aes(x = x[1], y = x[2]), pch = 3)
```
