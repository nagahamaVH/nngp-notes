---
title: "Generalized Gaussian Process Using NNGP"
author: "Victor Hugo Nagahama"
date: '2022-11-16'
output:
  pdf_document:
    toc: true
    number_sections: true
  html_document:
    toc: true
    number_sections: true
editor_options:
  chunk_output_type: console
---

***

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In order to illustrate the estimation of NNGP to Generalized Gaussian Process in the context of spatial data we will perform a simulation study

```{r}
library(ggplot2)
library(mvtnorm)
```

# Simulated data

\begin{align*}
  \boldsymbol{x} & \sim \mathcal{N} \left( \boldsymbol{0}, \boldsymbol{C}(\boldsymbol{\psi}) \right) \\
  y_i \mid \boldsymbol{x} & \overset{iid}\sim Poisson(\eta_i), \quad \eta_i = \exp x_i,
\end{align*}

\noindent
where the covariance matrix $C(\boldsymbol{\psi})$ is the Squared Exponential (SE) kernel in terms of signal variance and length-scale parameterization

\begin{equation*}
    \mathrm{C}(s_i, s_j) = \sigma^2 \exp \left\{ -\frac{(s_i - s_j)^2}{2 l^2} \right\}.
\end{equation*}

```{r}
n <- 500
m <- 3
set.seed(121)
coords <- cbind(runif(n), runif(n))

ord <- order(coords[,1])
coords <- coords[ord,]

sigma <- 3.2
l <- .7

d <- dist(coords) |>
  as.matrix()
# d <- dist(coords, diag = T, upper = T)
C <- sigma^2 * exp(-d / (2 * l^2))
x <- rmvnorm(1, sigma = C, method = "chol", checkSymmetry = F) |>
  c()
eta <- exp(x)
y <- rpois(n, eta)
```

The code above simulate `r n` locations in the spatial domain $\mathcal{D} = [0, 1]^2$ with covariance parameters $\sigma =$ `r round(sigma, 4)` and $l =$ `r round(l, 4)`.

```{r, echo = F}
df <- data.frame(d = d[,1], cov = C[,1])
ggplot(df, aes(d, cov)) +
  geom_line() +
  ggtitle("Correlation according to distance of spatial locations")
```

# Steps to estimate the model

1) Obtain the neighbourhood of each location
2) For a given vector of parameters compute the Laplace approximation
3) Compute the precision matrix
4) Iterate steps (2) and (3) until convergence of parameters


# Latent Gaussian Process inference

Let the vector of parameters $\boldsymbol{\theta}$, then for inference we need to evaluate the likelihood function which requires to solve an integral for non-Gaussian data. A convenient solution is to apply the second-order Taylor expansion (Laplace approximation) of log $\pi(\boldsymbol{y} \mid \boldsymbol{x})$ at the mode of posterior $\pi(\boldsymbol{x} \mid \boldsymbol{y})$.

\begin{equation*}
  \boldsymbol{\mathcal{L}}(\boldsymbol{\theta}) = \pi(\boldsymbol{y} \mid \boldsymbol{\theta}) = \int \pi(\boldsymbol{y} \mid \boldsymbol{x}, \boldsymbol{\theta}) \pi(\boldsymbol{x} \mid \boldsymbol{\theta}) d\boldsymbol{x}.
\end{equation*}

Iterative applications of the Taylor expansion under regular conditions converges to the desired multivariate normal distribution.

\begin{equation*}
  \tilde{\pi}(\boldsymbol{\theta} \mid \boldsymbol{y}) \propto \frac{\pi(\boldsymbol{y} \mid \boldsymbol{x}, \boldsymbol{\theta}) \pi (\boldsymbol{x} \mid \boldsymbol{\theta})}{|\boldsymbol{Q}_{\boldsymbol{x} \mid \boldsymbol{y}}|^{1/2}} \pi(\boldsymbol{\theta}).
\end{equation*}

To estimate this model we need to define the posterior distribution of the parameter, its gradient and hessian function.

```{r}
# pi(theta | x, y) \propto pi(y | x, theta) * pi(x | theta)
posterior <- function(y, x, sigma_x, log = T) {
  eta <- exp(x)
  ll <- sum(dpois(y, eta, log = T)) + dmvnorm(x, sigma = C, log = T, checkSymmetry = F)
  if (!log) {
    ll <- exp(ll)
  }
  return(ll)
}

grad <- function(x, y) {
  y - exp(x)
}

hess <- function(x) {
  -exp(x)
}
```

# NNGP

We can formalise the definition on NNGP as following. Consider a set of locations $\boldsymbol{S} = \{ \boldsymbol{s}_1, \boldsymbol{s}_2, \dots, \boldsymbol{s}_n \}$ where the data was sampled, for each location $\boldsymbol{s}_i$ we have a subset $\boldsymbol{c}(i)$ defined as the $m$ nearest neighbors of $\boldsymbol{s}_i$ then the approximation of $\pi(\boldsymbol{x})$ is given by

\begin{equation}
    \tilde{\pi}(\boldsymbol{x}) = \prod_{i=1}^n \pi \left( x_i \mid \boldsymbol{x}_{\boldsymbol{c}(i)} \right),
    \label{joint density of nngp}
\end{equation}

\begin{equation*}
    x_i \mid \boldsymbol{x}_{\boldsymbol{c}(i)} \sim \mathcal{N} \left( \boldsymbol{a}_i \boldsymbol{x}_{\boldsymbol{c}(i)}, \boldsymbol{D}_i \right),
\end{equation*}

\noindent
where, $\boldsymbol{a}_i = C(s_i, \boldsymbol{c}(i)) \left(C(\boldsymbol{c}(i), \boldsymbol{c}(i) \right)^{-1}$ and $\boldsymbol{D}_i = C(s_i, s_i) - \boldsymbol{a}_i^T C(\boldsymbol{c}(i), \boldsymbol{s}_i)$.

Moreover, each conditional density can be written as a linear combination of normal distributions as:

\begin{align*}
    x_1 & = \gamma_1 \\
    x_2 & = a_{21} x_1 + \gamma_2 \\
    x_3 & = a_{31} x_1 + a_{32} x_2 + \gamma_3
\end{align*}

where $\gamma_i \sim \mathcal{N}(0, d_i)$ with $d_1 = C(\boldsymbol{s}_1, \boldsymbol{s}_1)$. Which in the vector notation is

\begin{equation*}
    \boldsymbol{x} = \boldsymbol{x} \boldsymbol{A} + \boldsymbol{\gamma},
\end{equation*}

where $\boldsymbol{x} = (x_1, \dots, x_n)$, $\boldsymbol{\gamma} \sim \mathcal{N}(\boldsymbol{0}, \boldsymbol{D})$ with $\boldsymbol{D} = diag(d_1, \dots, d_n)$ and $\boldsymbol{A} = (a_{ij})$ is a lower triangular matrix. Therefore,

\begin{equation*}
    (\boldsymbol{I} - \boldsymbol{A}) \boldsymbol{x} = \boldsymbol{\gamma} \Leftrightarrow \boldsymbol{x} = (\boldsymbol{I} - \boldsymbol{A})^{-1} \boldsymbol{\gamma} \sim \mathcal{N} \left( \boldsymbol{0}, (\boldsymbol{I} - \boldsymbol{A})^{-1} \boldsymbol{D} (\boldsymbol{I} - \boldsymbol{A})^{-T} \right).
\end{equation*}

So the precision matrix is sparse and has a Cholesky decomposition $\boldsymbol{L}^T \boldsymbol{L}$.

\begin{align}
  \tilde{\boldsymbol{C}}^{-1} & = (\boldsymbol{I} - \boldsymbol{A})^T \boldsymbol{D} (\boldsymbol{I} - \boldsymbol{A}) \nonumber \\
  \boldsymbol{L} & = \boldsymbol{D}^{-1/2} (\boldsymbol{I} - \boldsymbol{A}). \label{cholesky}
\end{align}

## Getting the neighbourhood of each location

For simplicity we are using the function \texttt{spConjNNGP} from the package \texttt{spNNGP} to get the neighbourhood of each location.

```{r}
get_NN_ind <- function(ind, ind_distM_i, M){
  l <- ifelse(ind < M, ind, M)
  D_i <- rep(0, M);
  D_i[1:l] <- c(ind_distM_i[[ind]])[1:l]
  return(D_i)
}

get_nn <- function(coords, m){
  n <- dim(coords)[1]
  nn_data <- spNNGP::spConjNNGP(
    rep(0, n) ~ 1, coords = coords,
    n.neighbors = m,
    theta.alpha = c("phi" = 5, "alpha" = 0.5),
    sigma.sq.IG = c(2, 1),
    cov.model = "exponential",
    return.neighbor.info = T, fit.rep = F, 
    verbose = F)
  ord <- nn_data$neighbor.info$ord
  nn_idx <- sapply(1:(n - 1), get_NN_ind, nn_data$neighbor.info$n.indx[-1], m) |>
    t()
  
  return(list(ord = ord, nn_idx = nn_idx))
}
```

The code below return the Cholesky decomposition of a precision matrix from a NNGP model

```{r}
# Return t(L)L Cholesky decomposition of NNGP precision matrix
nngp_chol <- function(coords, m, p) {
  n <- dim(coords)[1]
  sigma <- p[1]
  l <- p[2]
  
  # Initialize A and dd = diag(D)
  A <- matrix(rep(0, n^2), n)
  dd <- c(sigma^2, rep(0, n - 1))
  
  # Cholesky
  nn_list <- get_nn(coords, m)
  
  # Parallelize
  for (i in 1:(n - 1)) {
    nn <- nn_list$nn_idx[i, ]
    nn <- nn[nn != 0]
    d <- dist(coords[c(i + 1, nn),]) |>
      as.matrix()
    C <- sigma^2 * exp(-d / (2 * l^2))
    A[i + 1, nn] <- solve(C[-1, -1], C[-1, 1])
    dd[i + 1] <- C[1, 1] - C[1, -1] %*% A[i + 1, nn]
  }
  
  Dinv <- diag(sqrt(1 / dd), nrow = n)
  L <- tcrossprod(Dinv, t(diag(nrow = n) - A))

  return(L)
}
```

The code bellow compute the Laplace Approximation

```{r}
# Laplace Approximation of pi(y | x)
la <- function(x0, y, Cinv, grad, hess, control = list(it = 100, tol = 1e-4)) {
  for (i in 1:control$it) {
    # Rearanging means
    H <- diag(hess(x0), length(x0))
    G <- grad(x0, y)
    Cx <- chol2inv(chol(Cinv - H)) # improve?
    mu <- as.vector(Cx %*% (G - H %*% x0))
    
    # N-R algorithm
    # U <- grad(x0, y)
    # D <- diag(-1 / hess(x0), length(x0))
    # Dinv <- diag(-hess(x0), length(x0))
    # Winv <- chol2inv(chol(Cinv + Dinv))
    # t <- x0 + D %*% U
    # mu <- Winv %*% (Dinv %*% t)
    
    if (max(abs(x0 - mu)) < control$tol) break
    if (i == control$it) stop("Max iteraction number reached")
    x0 <- mu
  }

  parms <- list(mu = mu, sigma = Cx)
  return(parms)
}
```

The code bellow returns the log likelihood of the NNGP which can be used in the \texttt{optim} function

```{r}
# Estimating parameters
la_nngp <- function(y, coords, p0, x0, grad, hess) {
  print(exp(p0))

  # Vecchia Approx
  L <- nngp_chol(coords, m, exp(p0))

  Cinv <- crossprod(L)
  C <- chol2inv(chol(Cinv))

  # Laplace Approximation
  gauss <- la(x0, y, Cinv, grad, hess)
  x_hat <- gauss$mu
  Cx <- gauss$sigma
  
  # Log-likelihood
  denom <- -(length(y) / 2) * log(2 * pi) - 0.5 * determinant(Cx)$modulus[1]
  ll <- posterior(y, x_hat, C) - denom
  return(ll)
}
```

```{r, eval = F}
p0 <- log(c(5, 5))
x0 <- log(y + .5)

fitted_parms <- optim(
  p0, la_nngp, y = y, coords = coords, x0 = x0, grad = grad, hess = hess,
  method = "Nelder-Mead", hessian = T, control = list(fnscale = -1))
```

```{r echo=FALSE, out.width = "80%"}
knitr::include_graphics("../images/par_poisson_nngp.png")
```

<!-------------------------- SANITY CHECKS ------------------------------->
```{r check_la_nngp_optim, eval = F, echo = F}
p0 <- log(c(5, 5))
x0 <- log(y + .5)

fitted_parms <- optim(
  p0, la_nngp, y = y, coords = coords, x0 = x0, grad = grad, hess = hess,
  method = "Nelder-Mead", hessian = T, control = list(fnscale = -1))

sigma_hat <- msm::deltamethod(
  list(~exp(x1), ~exp(x2)), fitted_parms$par, -solve(fitted_parms$hessian), 
  ses = F)

confint2 <- function(mu, sigma, alpha = 0.05) {
  q <- qnorm(alpha / 2) |> 
    abs()
  ub <- mu + q * sqrt(diag(sigma))
  lb <- mu - q * sqrt(diag(sigma))
  ci <- data.frame(ub, lb, mu)
  return(ci)
}

ci_parms <- confint2(exp(fitted_parms$par), sigma_hat) |>
  dplyr::mutate(
    par = c("sigma", "l"),
    true = c(sigma, l)
  )

ggplot(data = ci_parms, aes(x = "")) +
  facet_wrap(~par, scales = "free") +
  geom_hline(aes(yintercept = true), lty = 2) +
  geom_linerange(aes(ymin = lb, ymax = ub)) +
  geom_point(aes(y = mu)) +
  labs(y = "", x = "", title = "Estimated parameter")
# cowplot::save_plot(filename = "./images/par_poisson_nngp.png", plot = last_plot())
```

```{r check_chol, eval = F, echo = F}
n <- 50
m <- n - 1
set.seed(126)
coords <- cbind(runif(n), runif(n))

ord <- order(coords[,1])
coords <- coords[ord,]

sigma <- 2
l <- 1 / 2

d <- dist(coords) |>
  as.matrix()
# d <- dist(coords, diag = T, upper = T)
C <- sigma^2 * exp(-d / (2 * l^2))
x <- rmvnorm(1, sigma = C, method = "chol", checkSymmetry = F) |>
  c()
eta <- exp(x)
y <- rpois(n, eta)

Cinv <- chol2inv(chol(C))

L <- nngp_chol(coords, m, c(sigma, l))
Q_nngp <- crossprod(L)

lm(Q_nngp[lower.tri(Q_nngp)] ~ Cinv[lower.tri(Q_nngp)])
```

```{r check_la_marginal, eval = F, echo = F}
n <- 100
m <- 3
set.seed(126)
coords <- cbind(runif(n), runif(n))

ord <- order(coords[,1])
coords <- coords[ord,]

sigma <- 2
l <- 1 / 2

d <- dist(coords) |>
  as.matrix()
# d <- dist(coords, diag = T, upper = T)
C <- sigma^2 * exp(-d / (2 * l^2))
x <- rmvnorm(1, sigma = C, method = "chol", checkSymmetry = F) |>
  c()
eta <- exp(x)
y <- rpois(n, eta)

x0 <- log(y + .5)
Cinv <- chol2inv(chol(C))
fit_proxy <- la(x0, y, Cinv, grad, hess)

h <- 0.05
n_sample <- 6
set.seed(523)
sampled <- sample(1:n, n_sample, replace = F)
plots <- list()

posterior_marginal <- function(y, mu, x, C) {
  eta <- exp(mu)
  dpois(y, eta) * dmvnorm(x, sigma = C, checkSymmetry = F) 
}

for (i in 1:n_sample) {
  id <- sampled[i]
  se <- fit_proxy$sigma[id, id] |>
    sqrt()
  x_grid <- seq(fit_proxy$mu[id] - 3 * se, fit_proxy$mu[id] + 3 * se, by = h)
  
  df_plot <- rep(x, each = length(x_grid)) |>
    matrix(ncol = n) |>
    dplyr::as_tibble(.name_repair = make.names)
  df_plot[,id] <- x_grid
  names(df_plot) <- paste0("x", 1:n)
  
  unnorm_exact <- apply(df_plot, 1, posterior_marginal, y = y[id], mu = x[id], C = C)
  unnorm_proxy <- apply(df_plot, 1, dmvnorm, mean = fit_proxy$mu, sigma = fit_proxy$sigma, checkSymmetry = F) 
  
  df_plot <- df_plot |>
    dplyr::mutate(
      exact = unnorm_exact / sum(unnorm_exact),
      proxy = unnorm_proxy / sum(unnorm_proxy)
    )

  p <- ggplot(df_plot, aes_string(x = paste0("x", id))) +
    geom_line(aes(y = exact)) +
    geom_line(aes(y = proxy), linetype = 2, col = "red") +
    geom_point(data = data.frame(x = fit_proxy$mu[id]), aes(x = x, y = 0))
  plots[[i]] <- p
}

cowplot::plot_grid(plotlist = plots, nrow = 2)
```

```{r check_la_bivar, eval = F, echo = F}
n <- 50
m <- n - 1
set.seed(126)
coords <- cbind(runif(n), runif(n))

ord <- order(coords[,1])
coords <- coords[ord,]

sigma <- 2
l <- 1 / 2

d <- dist(coords) |>
  as.matrix()
# d <- dist(coords, diag = T, upper = T)
C <- sigma^2 * exp(-d / (2 * l^2))
x <- rmvnorm(10000, sigma = C, method = "chol", checkSymmetry = F) |>
  c()
x <- rmvnorm(10000, sigma = C, method = "chol", checkSymmetry = F) |>
  colMeans()
eta <- exp(x)
y <- rpois(n, eta)

x0 <- log(y + .5)
Cinv <- chol2inv(chol(C))
fit_proxy <- la(x0, y, Cinv, grad, hess)

df_plot <- NULL
w_grid <- list()
h <- 0.05
for (i in 1:n) {
  se <- fit_proxy$sigma[i, i] |>
    sqrt()
  w_grid[[i]] <- seq(fit_proxy$mu[i] - 3 * se, fit_proxy$mu[i] + 3 * se, by = h)
}

df_plot <- expand.grid(w_grid) |>
  as.data.frame()
names(df_plot) <- paste0("x", 1:n)

unnorm_exact <- apply(df_plot, 1, posterior, y = y, sigma_x = C, log = F)
unnorm_proxy <- apply(df_plot, 1, dmvnorm, fit_proxy$mu, fit_proxy$sigma) 

df_plot <- df_plot |>
  dplyr::mutate(
    exact = unnorm_exact / sum(unnorm_exact),
    proxy = unnorm_proxy / sum(unnorm_proxy)
  )

df_longer <- df_plot |>
  tidyr::pivot_longer(cols = c(exact, proxy), names_to = "type", values_to = "density")

ggplot(df_longer, aes(x = x1, y = x2)) +
  geom_contour(aes(z = density)) +
  facet_wrap(vars(type)) +
  geom_point(aes(x = x[1], y = x[2]), pch = 3)
```
